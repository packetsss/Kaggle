{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  id      feature_0      feature_1      feature_2  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean    99999.500000       0.972710       1.168365       2.219325   \n",
       "std     57735.171256       3.941836       3.993407       6.476570   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%     49999.750000       0.000000       0.000000       0.000000   \n",
       "50%     99999.500000       0.000000       0.000000       0.000000   \n",
       "75%    149999.250000       1.000000       1.000000       1.000000   \n",
       "max    199999.000000      61.000000      51.000000      64.000000   \n",
       "\n",
       "           feature_3      feature_4      feature_5      feature_6  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        2.296735       0.793530       1.431105       1.010695   \n",
       "std         7.551858       2.935785       5.162746       3.949231   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         1.000000       0.000000       1.000000       0.000000   \n",
       "max        70.000000      38.000000      76.000000      43.000000   \n",
       "\n",
       "           feature_7     feature_8  ...     feature_65     feature_66  \\\n",
       "count  200000.000000  200000.00000  ...  200000.000000  200000.000000   \n",
       "mean        0.673090       1.94398  ...       1.798040       0.508695   \n",
       "std         2.234949       3.93133  ...       5.053014       1.867330   \n",
       "min         0.000000       0.00000  ...       0.000000       0.000000   \n",
       "25%         0.000000       0.00000  ...       0.000000       0.000000   \n",
       "50%         0.000000       0.00000  ...       0.000000       0.000000   \n",
       "75%         0.000000       2.00000  ...       1.000000       0.000000   \n",
       "max        30.000000      38.00000  ...      54.000000      24.000000   \n",
       "\n",
       "          feature_67     feature_68     feature_69     feature_70  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        1.827300       0.910370       1.603585       1.219210   \n",
       "std         7.188924       3.835182       4.877679       4.826003   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         1.000000       1.000000       2.000000       1.000000   \n",
       "max        79.000000      55.000000      65.000000      67.000000   \n",
       "\n",
       "          feature_71     feature_72    feature_73     feature_74  \n",
       "count  200000.000000  200000.000000  200000.00000  200000.000000  \n",
       "mean        0.806895       1.282925       2.94021       0.632005  \n",
       "std         2.458741       4.261420      10.78465       3.925310  \n",
       "min         0.000000       0.000000       0.00000       0.000000  \n",
       "25%         0.000000       0.000000       0.00000       0.000000  \n",
       "50%         0.000000       0.000000       0.00000       0.000000  \n",
       "75%         1.000000       1.000000       1.00000       0.000000  \n",
       "max        30.000000      61.000000     130.00000      52.000000  \n",
       "\n",
       "[8 rows x 76 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>feature_0</th>\n      <th>feature_1</th>\n      <th>feature_2</th>\n      <th>feature_3</th>\n      <th>feature_4</th>\n      <th>feature_5</th>\n      <th>feature_6</th>\n      <th>feature_7</th>\n      <th>feature_8</th>\n      <th>...</th>\n      <th>feature_65</th>\n      <th>feature_66</th>\n      <th>feature_67</th>\n      <th>feature_68</th>\n      <th>feature_69</th>\n      <th>feature_70</th>\n      <th>feature_71</th>\n      <th>feature_72</th>\n      <th>feature_73</th>\n      <th>feature_74</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.00000</td>\n      <td>...</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.00000</td>\n      <td>200000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>99999.500000</td>\n      <td>0.972710</td>\n      <td>1.168365</td>\n      <td>2.219325</td>\n      <td>2.296735</td>\n      <td>0.793530</td>\n      <td>1.431105</td>\n      <td>1.010695</td>\n      <td>0.673090</td>\n      <td>1.94398</td>\n      <td>...</td>\n      <td>1.798040</td>\n      <td>0.508695</td>\n      <td>1.827300</td>\n      <td>0.910370</td>\n      <td>1.603585</td>\n      <td>1.219210</td>\n      <td>0.806895</td>\n      <td>1.282925</td>\n      <td>2.94021</td>\n      <td>0.632005</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>57735.171256</td>\n      <td>3.941836</td>\n      <td>3.993407</td>\n      <td>6.476570</td>\n      <td>7.551858</td>\n      <td>2.935785</td>\n      <td>5.162746</td>\n      <td>3.949231</td>\n      <td>2.234949</td>\n      <td>3.93133</td>\n      <td>...</td>\n      <td>5.053014</td>\n      <td>1.867330</td>\n      <td>7.188924</td>\n      <td>3.835182</td>\n      <td>4.877679</td>\n      <td>4.826003</td>\n      <td>2.458741</td>\n      <td>4.261420</td>\n      <td>10.78465</td>\n      <td>3.925310</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>49999.750000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>99999.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>149999.250000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.00000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>199999.000000</td>\n      <td>61.000000</td>\n      <td>51.000000</td>\n      <td>64.000000</td>\n      <td>70.000000</td>\n      <td>38.000000</td>\n      <td>76.000000</td>\n      <td>43.000000</td>\n      <td>30.000000</td>\n      <td>38.00000</td>\n      <td>...</td>\n      <td>54.000000</td>\n      <td>24.000000</td>\n      <td>79.000000</td>\n      <td>55.000000</td>\n      <td>65.000000</td>\n      <td>67.000000</td>\n      <td>30.000000</td>\n      <td>61.000000</td>\n      <td>130.00000</td>\n      <td>52.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 76 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential, layers\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "source": [
    "## Feature Scaling & Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"id\", axis=1, inplace=True)\n",
    "column_to_scale = [f\"feature_{i}\" for i in range(75)]\n",
    "\n",
    "df[column_to_scale] = MinMaxScaler().fit_transform(df[column_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {f\"Class_{i}\": i for i in range(1, 10)}\n",
    "df.target = [class_dict[x] for x in df.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           feature_0      feature_1      feature_2      feature_3  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.015946       0.022909       0.034677       0.032810   \n",
       "std         0.064620       0.078302       0.101196       0.107884   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.016393       0.019608       0.015625       0.014286   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "           feature_4      feature_5      feature_6      feature_7  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.020882       0.018830       0.023505       0.022436   \n",
       "std         0.077257       0.067931       0.091843       0.074498   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.013158       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "           feature_8      feature_9  ...     feature_66     feature_67  \\\n",
       "count  200000.000000  200000.000000  ...  200000.000000  200000.000000   \n",
       "mean        0.051157       0.023884  ...       0.021196       0.023130   \n",
       "std         0.103456       0.082180  ...       0.077805       0.090999   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "50%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "75%         0.052632       0.013889  ...       0.000000       0.012658   \n",
       "max         1.000000       1.000000  ...       1.000000       1.000000   \n",
       "\n",
       "          feature_68     feature_69     feature_70     feature_71  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.016552       0.024671       0.018197       0.026896   \n",
       "std         0.069731       0.075041       0.072030       0.081958   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.018182       0.030769       0.014925       0.033333   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "          feature_72     feature_73     feature_74         target  \n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000  \n",
       "mean        0.021032       0.022617       0.012154       5.973705  \n",
       "std         0.069859       0.082959       0.075487       2.475353  \n",
       "min         0.000000       0.000000       0.000000       1.000000  \n",
       "25%         0.000000       0.000000       0.000000       4.000000  \n",
       "50%         0.000000       0.000000       0.000000       6.000000  \n",
       "75%         0.016393       0.007692       0.000000       8.000000  \n",
       "max         1.000000       1.000000       1.000000       9.000000  \n",
       "\n",
       "[8 rows x 76 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0</th>\n      <th>feature_1</th>\n      <th>feature_2</th>\n      <th>feature_3</th>\n      <th>feature_4</th>\n      <th>feature_5</th>\n      <th>feature_6</th>\n      <th>feature_7</th>\n      <th>feature_8</th>\n      <th>feature_9</th>\n      <th>...</th>\n      <th>feature_66</th>\n      <th>feature_67</th>\n      <th>feature_68</th>\n      <th>feature_69</th>\n      <th>feature_70</th>\n      <th>feature_71</th>\n      <th>feature_72</th>\n      <th>feature_73</th>\n      <th>feature_74</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>...</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n      <td>200000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.015946</td>\n      <td>0.022909</td>\n      <td>0.034677</td>\n      <td>0.032810</td>\n      <td>0.020882</td>\n      <td>0.018830</td>\n      <td>0.023505</td>\n      <td>0.022436</td>\n      <td>0.051157</td>\n      <td>0.023884</td>\n      <td>...</td>\n      <td>0.021196</td>\n      <td>0.023130</td>\n      <td>0.016552</td>\n      <td>0.024671</td>\n      <td>0.018197</td>\n      <td>0.026896</td>\n      <td>0.021032</td>\n      <td>0.022617</td>\n      <td>0.012154</td>\n      <td>5.973705</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.064620</td>\n      <td>0.078302</td>\n      <td>0.101196</td>\n      <td>0.107884</td>\n      <td>0.077257</td>\n      <td>0.067931</td>\n      <td>0.091843</td>\n      <td>0.074498</td>\n      <td>0.103456</td>\n      <td>0.082180</td>\n      <td>...</td>\n      <td>0.077805</td>\n      <td>0.090999</td>\n      <td>0.069731</td>\n      <td>0.075041</td>\n      <td>0.072030</td>\n      <td>0.081958</td>\n      <td>0.069859</td>\n      <td>0.082959</td>\n      <td>0.075487</td>\n      <td>2.475353</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.016393</td>\n      <td>0.019608</td>\n      <td>0.015625</td>\n      <td>0.014286</td>\n      <td>0.000000</td>\n      <td>0.013158</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.052632</td>\n      <td>0.013889</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.012658</td>\n      <td>0.018182</td>\n      <td>0.030769</td>\n      <td>0.014925</td>\n      <td>0.033333</td>\n      <td>0.016393</td>\n      <td>0.007692</td>\n      <td>0.000000</td>\n      <td>8.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>9.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 76 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           feature_0      feature_1      feature_2      feature_3  \\\n",
       "count  180000.000000  180000.000000  180000.000000  180000.000000   \n",
       "mean        0.015937       0.022860       0.034680       0.032883   \n",
       "std         0.064588       0.078016       0.101173       0.107992   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.016393       0.019608       0.015625       0.014286   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "           feature_4      feature_5      feature_6      feature_7  \\\n",
       "count  180000.000000  180000.000000  180000.000000  180000.000000   \n",
       "mean        0.020888       0.018822       0.023408       0.022475   \n",
       "std         0.077424       0.067853       0.091396       0.074563   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.013158       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "           feature_8      feature_9  ...     feature_65     feature_66  \\\n",
       "count  180000.000000  180000.000000  ...  180000.000000  180000.000000   \n",
       "mean        0.051190       0.023841  ...       0.033199       0.021318   \n",
       "std         0.103535       0.081937  ...       0.093296       0.078336   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "50%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "75%         0.052632       0.013889  ...       0.018519       0.000000   \n",
       "max         1.000000       1.000000  ...       1.000000       1.000000   \n",
       "\n",
       "          feature_67     feature_68     feature_69     feature_70  \\\n",
       "count  180000.000000  180000.000000  180000.000000  180000.000000   \n",
       "mean        0.023068       0.016526       0.024644       0.018155   \n",
       "std         0.090831       0.069532       0.074882       0.072024   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.012658       0.018182       0.030769       0.014925   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "          feature_71     feature_72     feature_73     feature_74  \n",
       "count  180000.000000  180000.000000  180000.000000  180000.000000  \n",
       "mean        0.026937       0.021085       0.022618       0.012076  \n",
       "std         0.082009       0.070313       0.083039       0.075053  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       0.000000       0.000000  \n",
       "75%         0.033333       0.016393       0.007692       0.000000  \n",
       "max         1.000000       1.000000       1.000000       1.000000  \n",
       "\n",
       "[8 rows x 75 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0</th>\n      <th>feature_1</th>\n      <th>feature_2</th>\n      <th>feature_3</th>\n      <th>feature_4</th>\n      <th>feature_5</th>\n      <th>feature_6</th>\n      <th>feature_7</th>\n      <th>feature_8</th>\n      <th>feature_9</th>\n      <th>...</th>\n      <th>feature_65</th>\n      <th>feature_66</th>\n      <th>feature_67</th>\n      <th>feature_68</th>\n      <th>feature_69</th>\n      <th>feature_70</th>\n      <th>feature_71</th>\n      <th>feature_72</th>\n      <th>feature_73</th>\n      <th>feature_74</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>...</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n      <td>180000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.015937</td>\n      <td>0.022860</td>\n      <td>0.034680</td>\n      <td>0.032883</td>\n      <td>0.020888</td>\n      <td>0.018822</td>\n      <td>0.023408</td>\n      <td>0.022475</td>\n      <td>0.051190</td>\n      <td>0.023841</td>\n      <td>...</td>\n      <td>0.033199</td>\n      <td>0.021318</td>\n      <td>0.023068</td>\n      <td>0.016526</td>\n      <td>0.024644</td>\n      <td>0.018155</td>\n      <td>0.026937</td>\n      <td>0.021085</td>\n      <td>0.022618</td>\n      <td>0.012076</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.064588</td>\n      <td>0.078016</td>\n      <td>0.101173</td>\n      <td>0.107992</td>\n      <td>0.077424</td>\n      <td>0.067853</td>\n      <td>0.091396</td>\n      <td>0.074563</td>\n      <td>0.103535</td>\n      <td>0.081937</td>\n      <td>...</td>\n      <td>0.093296</td>\n      <td>0.078336</td>\n      <td>0.090831</td>\n      <td>0.069532</td>\n      <td>0.074882</td>\n      <td>0.072024</td>\n      <td>0.082009</td>\n      <td>0.070313</td>\n      <td>0.083039</td>\n      <td>0.075053</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.016393</td>\n      <td>0.019608</td>\n      <td>0.015625</td>\n      <td>0.014286</td>\n      <td>0.000000</td>\n      <td>0.013158</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.052632</td>\n      <td>0.013889</td>\n      <td>...</td>\n      <td>0.018519</td>\n      <td>0.000000</td>\n      <td>0.012658</td>\n      <td>0.018182</td>\n      <td>0.030769</td>\n      <td>0.014925</td>\n      <td>0.033333</td>\n      <td>0.016393</td>\n      <td>0.007692</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 75 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "X, y = df.drop(\"target\", axis=1), df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=5)\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((180000, 5, 5, 3), (180000,))"
      ]
     },
     "metadata": {},
     "execution_count": 162
    }
   ],
   "source": [
    "X_train = np.array([X_train]).reshape(-1, 5, 5, 3)\n",
    "X_test = np.array([X_test]).reshape(-1, 5, 5, 3)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # Preprocessing\n",
    "    # preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "\n",
    "    # cnn\n",
    "    layers.Conv1D(8, 1, activation='relu'),\n",
    "\n",
    "    layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=(5, 5, 3)),\n",
    "    # layers.BatchNormalization(axis=1),\n",
    "\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    # layers.BatchNormalization(axis=1),\n",
    "\n",
    "    # output\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(units=128,activation='relu'),\n",
    "    layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "5625/5625 [==============================] - 13s 2ms/step - loss: 1.8346 - accuracy: 0.3225\n",
      "Epoch 2/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7609 - accuracy: 0.3566\n",
      "Epoch 3/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7599 - accuracy: 0.3568\n",
      "Epoch 4/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7556 - accuracy: 0.3573\n",
      "Epoch 5/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.7521 - accuracy: 0.3582\n",
      "Epoch 6/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7508 - accuracy: 0.3589\n",
      "Epoch 7/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.7498 - accuracy: 0.3600\n",
      "Epoch 8/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7485 - accuracy: 0.3626\n",
      "Epoch 9/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7426 - accuracy: 0.3636\n",
      "Epoch 10/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.7415 - accuracy: 0.3623\n",
      "Epoch 11/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7419 - accuracy: 0.3645\n",
      "Epoch 12/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7403 - accuracy: 0.3645\n",
      "Epoch 13/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.7374 - accuracy: 0.3662\n",
      "Epoch 14/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7359 - accuracy: 0.3672\n",
      "Epoch 15/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.7365 - accuracy: 0.3663\n",
      "Epoch 16/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.7342 - accuracy: 0.3674\n",
      "Epoch 17/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7316 - accuracy: 0.3679\n",
      "Epoch 18/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7296 - accuracy: 0.3677\n",
      "Epoch 19/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7336 - accuracy: 0.3689\n",
      "Epoch 20/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7344 - accuracy: 0.3672\n",
      "Epoch 1/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7296 - accuracy: 0.3689\n",
      "Epoch 2/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.7288 - accuracy: 0.3695\n",
      "Epoch 3/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7279 - accuracy: 0.3702\n",
      "Epoch 4/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7263 - accuracy: 0.3713\n",
      "Epoch 5/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7254 - accuracy: 0.3712\n",
      "Epoch 6/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7241 - accuracy: 0.3706\n",
      "Epoch 7/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7234 - accuracy: 0.3722\n",
      "Epoch 8/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7225 - accuracy: 0.3719\n",
      "Epoch 9/20\n",
      "5625/5625 [==============================] - 13s 2ms/step - loss: 1.7209 - accuracy: 0.3722\n",
      "Epoch 10/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7203 - accuracy: 0.3728\n",
      "Epoch 11/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7192 - accuracy: 0.3731\n",
      "Epoch 12/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7180 - accuracy: 0.3737\n",
      "Epoch 13/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7170 - accuracy: 0.3736\n",
      "Epoch 14/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7169 - accuracy: 0.3736\n",
      "Epoch 15/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7163 - accuracy: 0.3740\n",
      "Epoch 16/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7155 - accuracy: 0.3745\n",
      "Epoch 17/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.7143 - accuracy: 0.3748\n",
      "Epoch 18/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7138 - accuracy: 0.3745\n",
      "Epoch 19/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7129 - accuracy: 0.3752\n",
      "Epoch 20/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7116 - accuracy: 0.3752\n",
      "Epoch 1/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7116 - accuracy: 0.3758\n",
      "Epoch 2/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7104 - accuracy: 0.3759\n",
      "Epoch 3/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7095 - accuracy: 0.3763\n",
      "Epoch 4/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7088 - accuracy: 0.3765\n",
      "Epoch 5/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7086 - accuracy: 0.3761\n",
      "Epoch 6/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7088 - accuracy: 0.3764\n",
      "Epoch 7/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7068 - accuracy: 0.3765\n",
      "Epoch 8/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7062 - accuracy: 0.3775\n",
      "Epoch 9/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7058 - accuracy: 0.3774\n",
      "Epoch 10/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7051 - accuracy: 0.3779\n",
      "Epoch 11/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7045 - accuracy: 0.3783\n",
      "Epoch 12/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7043 - accuracy: 0.3784\n",
      "Epoch 13/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7030 - accuracy: 0.3779\n",
      "Epoch 14/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7027 - accuracy: 0.3779\n",
      "Epoch 15/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7027 - accuracy: 0.3778\n",
      "Epoch 16/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7023 - accuracy: 0.3786\n",
      "Epoch 17/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.7006 - accuracy: 0.3798\n",
      "Epoch 18/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.7010 - accuracy: 0.3786\n",
      "Epoch 19/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6998 - accuracy: 0.3788\n",
      "Epoch 20/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6997 - accuracy: 0.3791\n",
      "Epoch 1/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6991 - accuracy: 0.3794\n",
      "Epoch 2/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6975 - accuracy: 0.3808\n",
      "Epoch 3/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6974 - accuracy: 0.3804\n",
      "Epoch 4/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6980 - accuracy: 0.3795\n",
      "Epoch 5/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6978 - accuracy: 0.3804\n",
      "Epoch 6/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6968 - accuracy: 0.3809\n",
      "Epoch 7/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6970 - accuracy: 0.3802\n",
      "Epoch 8/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6961 - accuracy: 0.3810\n",
      "Epoch 9/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6968 - accuracy: 0.3810\n",
      "Epoch 10/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6959 - accuracy: 0.3802\n",
      "Epoch 11/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6960 - accuracy: 0.3811\n",
      "Epoch 12/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6948 - accuracy: 0.3813\n",
      "Epoch 13/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6946 - accuracy: 0.3817\n",
      "Epoch 14/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6943 - accuracy: 0.3816\n",
      "Epoch 15/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6936 - accuracy: 0.3813\n",
      "Epoch 16/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6930 - accuracy: 0.3811\n",
      "Epoch 17/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6927 - accuracy: 0.3814\n",
      "Epoch 18/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6929 - accuracy: 0.3818\n",
      "Epoch 19/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6930 - accuracy: 0.3816\n",
      "Epoch 20/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6929 - accuracy: 0.3809\n",
      "Epoch 1/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6907 - accuracy: 0.3824\n",
      "Epoch 2/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6912 - accuracy: 0.3825\n",
      "Epoch 3/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6910 - accuracy: 0.3842\n",
      "Epoch 4/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6917 - accuracy: 0.3816\n",
      "Epoch 5/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6907 - accuracy: 0.3820\n",
      "Epoch 6/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6897 - accuracy: 0.3830\n",
      "Epoch 7/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6889 - accuracy: 0.3838\n",
      "Epoch 8/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6885 - accuracy: 0.3835\n",
      "Epoch 9/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6894 - accuracy: 0.3838\n",
      "Epoch 10/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6901 - accuracy: 0.3826\n",
      "Epoch 11/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6905 - accuracy: 0.3828\n",
      "Epoch 12/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6886 - accuracy: 0.3837\n",
      "Epoch 13/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6883 - accuracy: 0.3834\n",
      "Epoch 14/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6878 - accuracy: 0.3844\n",
      "Epoch 15/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6882 - accuracy: 0.3837\n",
      "Epoch 16/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6883 - accuracy: 0.3836\n",
      "Epoch 17/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6872 - accuracy: 0.3826\n",
      "Epoch 18/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6887 - accuracy: 0.3835\n",
      "Epoch 19/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6873 - accuracy: 0.3842\n",
      "Epoch 20/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6873 - accuracy: 0.3839\n",
      "Epoch 1/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6869 - accuracy: 0.3843\n",
      "Epoch 2/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6856 - accuracy: 0.3841\n",
      "Epoch 3/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6863 - accuracy: 0.3837\n",
      "Epoch 4/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6864 - accuracy: 0.3841\n",
      "Epoch 5/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6861 - accuracy: 0.3839\n",
      "Epoch 6/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6861 - accuracy: 0.3835\n",
      "Epoch 7/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6855 - accuracy: 0.3837\n",
      "Epoch 8/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6846 - accuracy: 0.3838\n",
      "Epoch 9/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6859 - accuracy: 0.3839\n",
      "Epoch 10/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6861 - accuracy: 0.3837\n",
      "Epoch 11/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6840 - accuracy: 0.3849\n",
      "Epoch 12/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6855 - accuracy: 0.3843\n",
      "Epoch 13/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6862 - accuracy: 0.3835\n",
      "Epoch 14/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6840 - accuracy: 0.3851\n",
      "Epoch 15/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6845 - accuracy: 0.3851\n",
      "Epoch 16/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6845 - accuracy: 0.3844\n",
      "Epoch 17/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6829 - accuracy: 0.3849\n",
      "Epoch 18/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6843 - accuracy: 0.3847\n",
      "Epoch 19/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6849 - accuracy: 0.3839\n",
      "Epoch 20/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6849 - accuracy: 0.3849\n",
      "Epoch 1/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6847 - accuracy: 0.3837\n",
      "Epoch 2/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6836 - accuracy: 0.3857\n",
      "Epoch 3/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6840 - accuracy: 0.3848\n",
      "Epoch 4/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6843 - accuracy: 0.3846\n",
      "Epoch 5/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6825 - accuracy: 0.3848\n",
      "Epoch 6/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6826 - accuracy: 0.3856\n",
      "Epoch 7/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6838 - accuracy: 0.3851\n",
      "Epoch 8/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6835 - accuracy: 0.3850\n",
      "Epoch 9/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6842 - accuracy: 0.3850\n",
      "Epoch 10/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6824 - accuracy: 0.3863\n",
      "Epoch 11/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6825 - accuracy: 0.3851\n",
      "Epoch 12/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6820 - accuracy: 0.3851\n",
      "Epoch 13/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6814 - accuracy: 0.3854\n",
      "Epoch 14/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6829 - accuracy: 0.3855\n",
      "Epoch 15/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6817 - accuracy: 0.3857\n",
      "Epoch 16/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6829 - accuracy: 0.3855\n",
      "Epoch 17/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6836 - accuracy: 0.3853\n",
      "Epoch 18/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6825 - accuracy: 0.3854\n",
      "Epoch 19/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6826 - accuracy: 0.3851\n",
      "Epoch 20/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6818 - accuracy: 0.3865\n",
      "Epoch 1/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6816 - accuracy: 0.3854\n",
      "Epoch 2/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6825 - accuracy: 0.3852\n",
      "Epoch 3/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6817 - accuracy: 0.3860\n",
      "Epoch 4/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6817 - accuracy: 0.3862\n",
      "Epoch 5/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6806 - accuracy: 0.3862\n",
      "Epoch 6/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6815 - accuracy: 0.3852\n",
      "Epoch 7/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6810 - accuracy: 0.3856\n",
      "Epoch 8/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6813 - accuracy: 0.3854\n",
      "Epoch 9/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6811 - accuracy: 0.3855\n",
      "Epoch 10/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6821 - accuracy: 0.3862\n",
      "Epoch 11/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6818 - accuracy: 0.3854\n",
      "Epoch 12/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6799 - accuracy: 0.3860\n",
      "Epoch 13/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6823 - accuracy: 0.3861\n",
      "Epoch 14/20\n",
      "5625/5625 [==============================] - 11s 2ms/step - loss: 1.6811 - accuracy: 0.3870\n",
      "Epoch 15/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6799 - accuracy: 0.3867\n",
      "Epoch 16/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6809 - accuracy: 0.3867\n",
      "Epoch 17/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6813 - accuracy: 0.3860\n",
      "Epoch 18/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6827 - accuracy: 0.3853\n",
      "Epoch 19/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6799 - accuracy: 0.3860\n",
      "Epoch 20/20\n",
      "5625/5625 [==============================] - 12s 2ms/step - loss: 1.6794 - accuracy: 0.3867\n",
      "3min 53s ± 1.41 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           1       0.10      0.00      0.00       914\n           2       0.29      0.46      0.36      2329\n           3       0.08      0.00      0.00      1461\n           4       0.00      0.00      0.00       487\n           5       0.00      0.00      0.00       329\n           6       0.37      0.54      0.44      5129\n           7       0.00      0.00      0.00      1482\n           8       0.35      0.58      0.44      5248\n           9       0.13      0.01      0.01      2621\n\n    accuracy                           0.34     20000\n   macro avg       0.15      0.18      0.14     20000\nweighted avg       0.25      0.34      0.27     20000\n\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = [np.argmax(x) for x in y_pred]\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.04761905, 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.02884615, 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.01639344, 0.03921569, 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.03030303, 0.        ],\n",
       "         [0.05405405, 0.04651163, 0.        ]],\n",
       "\n",
       "        [[0.        , 0.03703704, 0.        ],\n",
       "         [0.        , 0.        , 0.06666667],\n",
       "         [0.        , 0.        , 0.04545455],\n",
       "         [0.        , 0.02013423, 0.        ],\n",
       "         [0.01190476, 0.        , 0.        ]],\n",
       "\n",
       "        [[0.09090909, 0.        , 0.        ],\n",
       "         [0.02439024, 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.01234568, 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.03333333, 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.01785714],\n",
       "         [0.        , 0.02631579, 0.08333333],\n",
       "         [0.01923077, 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.0125    , 0.06862745],\n",
       "         [0.        , 0.04      , 0.05555556],\n",
       "         [0.04166667, 0.03797468, 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.04918033, 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.01960784, 0.109375  ],\n",
       "         [0.01428571, 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.15789474],\n",
       "         [0.        , 0.03030303, 0.        ],\n",
       "         [0.        , 0.        , 0.03125   ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.07142857],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.04545455],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.01904762, 0.        ]],\n",
       "\n",
       "        [[0.        , 0.1025641 , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.01234568, 0.        ],\n",
       "         [0.        , 0.06349206, 0.        ]],\n",
       "\n",
       "        [[0.        , 0.00854701, 0.        ],\n",
       "         [0.        , 0.        , 0.10714286],\n",
       "         [0.        , 0.        , 0.05555556],\n",
       "         [0.09615385, 0.        , 0.        ],\n",
       "         [0.        , 0.06666667, 0.00284091]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.05555556],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.04477612, 0.        ],\n",
       "         [0.03278689, 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.03125   ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.03333333],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.04545455, 0.02564103, 0.        ],\n",
       "         [0.02439024, 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.01265823, 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.03846154]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.05263158, 0.02631579],\n",
       "         [0.        , 0.        , 0.02631579],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.24324324, 0.02325581, 0.03125   ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.04545455, 0.00380228, 0.        ],\n",
       "         [0.3030303 , 0.        , 0.04545455],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.09090909, 0.02564103, 0.01282051],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.02941176, 0.        ],\n",
       "         [0.04081633, 0.01234568, 0.        ],\n",
       "         [0.01886792, 0.01587302, 0.        ]],\n",
       "\n",
       "        [[0.1       , 0.01709402, 0.01030928],\n",
       "         [0.025     , 0.02631579, 0.03571429],\n",
       "         [0.        , 0.        , 0.11111111],\n",
       "         [0.05769231, 0.        , 0.19565217],\n",
       "         [0.        , 0.03333333, 0.        ]],\n",
       "\n",
       "        [[0.01298701, 0.025     , 0.        ],\n",
       "         [0.0125    , 0.        , 0.01851852],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.01492537, 0.        ],\n",
       "         [0.01639344, 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.13953488, 0.        , 0.02631579],\n",
       "         [0.15277778, 0.        , 0.        ],\n",
       "         [0.02702703, 0.02325581, 0.03125   ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.00380228, 0.06666667],\n",
       "         [0.15151515, 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.03571429]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.02439024, 0.02777778, 0.        ],\n",
       "         [0.04761905, 0.08823529, 0.        ],\n",
       "         [0.        , 0.02469136, 0.        ],\n",
       "         [0.        , 0.03174603, 0.        ]],\n",
       "\n",
       "        [[0.        , 0.00854701, 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.02631579, 0.        ],\n",
       "         [0.00961538, 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.00284091]],\n",
       "\n",
       "        [[0.00865801, 0.        , 0.01960784],\n",
       "         [0.        , 0.04      , 0.        ],\n",
       "         [0.08333333, 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 180
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_test.drop(\"id\", axis=1, inplace=True)\n",
    "column_to_scale = [f\"feature_{i}\" for i in range(75)]\n",
    "df_test[column_to_scale] = MinMaxScaler().fit_transform(df_test[column_to_scale])\n",
    "df_test = np.array([df_test]).reshape(-1, 5, 5, 3)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = model.predict(df_test)\n",
    "# y_pred_final = [np.argmax(x) for x in y_pred_final]\n",
    "\n",
    "# df_result = pd.DataFrame({\"ImageId\": range(200000, 300000), \"Label\": y_pred_final})\n",
    "# df_result.to_csv(\"result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " ...]"
      ]
     },
     "metadata": {},
     "execution_count": 182
    }
   ],
   "source": [
    "y_pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}